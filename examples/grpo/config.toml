spec_class = "training_wizard.recipes.grpo.GRPOTrainingSpec"
sample_rate = 100

[[reward_specs]]
spec_class = "examples.grpo.reward.GECExampleRewardSpec"

[transformer_spec]
pretrained_name = "llamafactory/tiny-random-Llama-3"

[dataset_spec]
spec_class = "training_wizard.specs.dataset.TemplateInstructDatasourceSpec"
output_column = "prompt"

[[dataset_spec.messages_template]]
role = "system"
content = """You are a grammar correction assistant. Fix any grammatical errors in the user's input while preserving the meaning."""

[[dataset_spec.messages_template]]
role = "user"
content = """{source}"""

[dataset_spec.parent]
spec_class = "training_wizard.specs.dataset.SimpleDataSourceSpec"
data_files = "examples/data/seq2seq.jsonl"

[training_args_spec]
run_name = "My Example GRPO"
output_dir = "examples/grpo/output"

warmup_steps = 0
logging_steps = 1
num_train_epochs = 1

per_device_train_batch_size = 2

bf16 = true
optim = "adamw_bnb_8bit"
learning_rate = 1e-6

eval_strategy = "no"
save_strategy = "steps"
save_steps = 1000
load_best_model_at_end = false

gradient_checkpointing = true
save_only_model = true

# --- New fields required by GRPOConfig ---
seed = 42
gradient_accumulation_steps = 1
max_prompt_length = 256
max_completion_length = 128
num_generations = 2
temperature = 1.0
num_iterations = 1
beta = 0.0
epsilon = 0.2
# log_completions = true # <- not needed because we use custom logging

# --- VLLM ---
# Using vLLM requires starting a server manually now, see:
# https://huggingface.co/docs/trl/main/en/grpo_trainer#speed-up-training-with-vllm-powered-generation
