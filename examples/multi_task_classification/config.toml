spec_class = "training_wizard.recipes.modular.ModularTrainingSpec"
validation = 6

[wizard_module]
spec_class = "training_wizard.specs.modules.multi_task_seq_cls.MultiTaskSequenceClassifierModule"
group_sizes = [
    3,
    2,
    4,
] # First task: 3 classes, second task: 2 classes, third task: 4 classes
label_smoothing = 0.1
compute_eval_metrics = true
show_per_group_metrics = true

[wizard_module.transformer_spec]
pretrained_name = "microsoft/deberta-v3-xsmall"

[wizard_module.transformer_spec.from_pretrained_kwargs]
num_labels = 9 # Sum of group_sizes: 3 + 2 + 4 = 9

[wizard_module.transformer_spec.tokenizer_call_kwargs]
max_length = 256
truncation = true

[dataset_spec]
spec_class = "training_wizard.specs.dataset.SimpleDataSourceSpec"
data_files = "examples/data/multi_task_classification.jsonl"

[training_args_spec]
output_dir = "examples/multi_task_classification/output"
run_name = "Multi-Task Classification Example"
num_train_epochs = 2
warmup_steps = 200
logging_steps = 50
per_device_train_batch_size = 4
per_device_eval_batch_size = 4
bf16 = true
fp16 = false
optim = "adamw_bnb_8bit"
learning_rate = 2e-5
weight_decay = 0.01
gradient_accumulation_steps = 1
gradient_checkpointing = false
save_only_model = true

[callbacks_spec]
enable_early_stopping = true
early_stopping_patience = 3
evaluate_first_step = true
