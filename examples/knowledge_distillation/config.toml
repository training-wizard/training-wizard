spec_class = "training_wizard.recipes.modular.ModularTrainingSpec"
validation = 6

[wizard_module]
spec_class = "training_wizard.specs.modules.knowledge_distillation.KnowledgeDistillationModule"
temperature = 3.0
distillation_weight = 0.7

[wizard_module.teacher_spec]
pretrained_name = "llamafactory/tiny-random-Llama-3"

[wizard_module.teacher_spec.tokenizer_call_kwargs]
max_length = 512
truncation = true

[wizard_module.student_module]
spec_class = "training_wizard.specs.modules.instruction_tune.InstructionTuningModule"

[wizard_module.student_module.transformer_spec]
pretrained_name = "llamafactory/tiny-random-Llama-3"

[wizard_module.student_module.transformer_spec.tokenizer_call_kwargs]
max_length = 512
truncation = true

[dataset_spec]
spec_class = "training_wizard.specs.dataset.TemplateInstructDatasourceSpec"
output_column = "messages"

[[dataset_spec.messages_template]]
role = "user"
content = """{source}"""

[[dataset_spec.messages_template]]
role = "assistant"
content = """{target}"""

[dataset_spec.parent]
spec_class = "training_wizard.specs.dataset.SimpleDataSourceSpec"
data_files = "examples/data/seq2seq.jsonl"

[dataset_spec.parent.preprocessing]
take = 50 # Limit for demonstration

[training_args_spec]
output_dir = "examples/knowledge_distillation/output"
run_name = "Knowledge Distillation Example"
num_train_epochs = 1
warmup_steps = 100
logging_steps = 10
per_device_train_batch_size = 1
per_device_eval_batch_size = 1
bf16 = true
fp16 = false
optim = "adamw_bnb_8bit"
learning_rate = 5e-6
weight_decay = 0.01
gradient_accumulation_steps = 4
gradient_checkpointing = true
save_only_model = true

[callbacks_spec]
enable_early_stopping = true
early_stopping_patience = 2
evaluate_first_step = false
