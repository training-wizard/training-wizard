{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Training Wizard","text":"<p> An alchemy of automation for the modern mage of machine learning </p> <p>The training wizard is a spec-driven tool for quick prototyping and finetuning of transformers models.</p>"},{"location":"#overview","title":"Overview","text":"<p>The Training Wizard is a tool that streamlines machine learning workflows by providing reusable components, structured experimentation, and TOML-based configuration for rapid model development and deployment.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Reusable Components: Shared library of ML functions and utilities</li> <li>Experiment Tracking: Built-in MLflow integration for consistent operations</li> <li>Simple Configuration: TOML-based setup for streamlined model finetuning</li> <li>Multi-GPU Support: Accelerate integration for distributed training</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"<ul> <li>New Projects: Offers a templated approach for setting up new ML experiments quickly and efficiently.</li> <li>Model Training: Abstracts complex details like parallelization and quantization, allowing users to concentrate on problem-solving.</li> <li>Experiment Management: Integrates seamless tracking of experiments and models, ensuring results are readily accessible.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<ol> <li><code>git clone</code> the project repository to create a working copy on your machine.</li> <li>Ensure that you have uv installed. The official documentation for installing uv has detailed instructions.</li> <li>Run <code>uv sync --all-groups --all-extras</code>.</li> </ol>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Each example in the <code>examples/</code> directory contains a complete recipe with its own README. Browse the available recipes and adapt one to your use case:</p> <ul> <li>Instruction Tuning: Fine-tune models for instruction following</li> <li>Sequence Classification: Train classification models</li> <li>GRPO/CPO/DPO: Preference optimization techniques</li> <li>Quantization: Efficient model compression</li> <li>And more...</li> </ul> <p>Run any recipe with:</p> <pre><code>training-wizard examples/[recipe]/config.toml\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! If you've built something that would improve Training Wizard, please open a pull request. Please read first the developer documentation and the legal terms that apply to the project.</p>"},{"location":"legal_terms/","title":"Legal Terms","text":""},{"location":"legal_terms/#source-code","title":"Source Code","text":"<p>The source code in this project is licensed under the Apache License 2.0. This is a permissive open source license that allows you to freely use, modify, and distribute the code, provided that you include a copy of the license and provide proper attribution. It also provides an express grant of patent rights from contributors.</p> <p>For more details, see the Apache 2.0 license page.</p>"},{"location":"legal_terms/#documentation","title":"Documentation","text":"<p>The documentation for this project is licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) license.</p> <p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format  </li> <li>Adapt \u2014 remix, transform, and build upon the material for any purpose, even commercially  </li> </ul> <p>as long as you give appropriate credit, provide a link to the license, and indicate if changes were made. For more details, see the CC BY 4.0 license page.</p>"},{"location":"concepts/","title":"Concepts","text":"<p>This tome will guide you through the ways of the Training Wizard. Each page covers a key aspect of the library.</p>"},{"location":"concepts/complexity/","title":"Managing Complexity in Model Training","text":""},{"location":"concepts/complexity/#the-problem","title":"The Problem","text":"<p>The complexity of training language models, especially during the prototype stage, can be a daunting task. Altering one element often has a cascading effect on others, which can lead to unmaintainable codebases and overly complicated workflows.</p>"},{"location":"concepts/complexity/#the-solution","title":"The Solution","text":"<p>We take a page from the software engineer's handbook to handle this complexity through strategic layers of abstraction. By abstracting away details at different levels, individuals working on a project can focus on their specific area without being overwhelmed by the entire system.</p>"},{"location":"concepts/recipe/","title":"Recipes","text":"<p>Recipes are the endpoints of the Training Wizard. They are, in essence, specs that make things happen - usually train and save a new model.</p>"},{"location":"concepts/recipe/#recipes-are-iterative","title":"Recipes are Iterative","text":"<p>To tackle the complexity of building flexible yet consistent training pipelines, recipes are rather object-oriented. That is, they inherit both functionality and attributes from their base classes.</p> <p>In this way, we can focus on implementing only the relevant part of each recipe while also ensuring that specs which are shared across recipes are handled the same way, reducing the number of surprises for the user. </p> <p>Starting with high-level abstractions such as <code>TrainingRecipe</code> and gradually building towards concrete recipes, the Training Wizard offers a clear path through the maze of possible variable components.</p>"},{"location":"concepts/spec/","title":"What is a spec?","text":"<p>In the context of the Training Wizard, a spec (short for specification) is a type of Python class that can be fully initialized by a config file.</p> <p>Here's a simple example of a spec:</p> <pre><code>class LoraConfigSpec(...):\n    \"\"\"The specification for the LoRA model. Mimics the HuggingFace config.\"\"\"\n\n    r: int = 32\n    \"\"\"r parameter for the LoRA model\"\"\"\n\n    lora_alpha: int = 64\n    \"\"\"Alpha parameter for the LoRA model. 2*r is a good default value.\"\"\"\n\n    lora_dropout: float = 0.05\n    \"\"\"The dropout of the LoRA layers\"\"\"\n\n    bias: Literal[\"none\", \"all\", \"lora_only\"] = \"none\"\n    \"\"\"The type of bias to use in the LoRA model. Must be one of:\n    none\n    all\n    lora_only\n    \"\"\"\n\n    target_modules: list[str] | str = \"all-linear\"\n    \"\"\"The modules to apply LoRA to. Must be a non-empty list.\"\"\"\n</code></pre> <p>As it is, this class is really just a dataclass with some fields. We can easily initialize it from a config file, though:</p> config.toml<pre><code>r = 4\nlora_dropout = 0.1\ntarget_modules = [\"q\", \"k\", \"v\"]\n</code></pre> load_lora.py<pre><code>import tomllib\n\nwith open(\"config.toml\") as f:\n    config = tomllib.loads(f.read())\n    lora = LoraConfigSpec(**config)\n</code></pre> <p>Tip</p> <p>Note that some fields have default values, so we may choose to omit them.</p> <p>But what if these specs could do more? Hmm. \ud83e\udd14</p>"},{"location":"concepts/spec/#more-than-just-data-containers","title":"More than just data containers","text":"<p>Specs in the Training Wizard are powered by Pydantic, a powerful data validation library that is great at - you guessed it - validating data. </p>"},{"location":"concepts/spec/#field-validators","title":"Field Validators","text":"<p>Let's ensure that the list of <code>target_modules</code> will never be empty by declaring our constraint as a field validator:</p> <pre><code>class LoraConfigSpec(...):\n    ... # same fields as before\n\n    @field_validator(\"target_modules\")\n    @classmethod\n    def target_modules_not_empty(cls, v: List[str]) -&gt; List[str]:\n        \"\"\"Validate that the target modules are valid.\"\"\"\n        assert len(v) &gt; 0, \"target_modules must be a non-empty list\"\n        return v\n</code></pre> <p>Most of this code is boilerplate. All you need to know is that you can define validators for certain fields that can assert properties at init time.</p>"},{"location":"concepts/spec/#model-validators","title":"Model Validators","text":"<p>Sometimes, we may want to check that combinations of fields work together. For this, we use model validators, which also have access to self, the class instance. Let's assume that, for some reason, we always wanted <code>lora_alpha</code> to be twice as high as <code>r</code>. Here's how we would do that:</p> <pre><code>class LoraConfigSpec(...):\n    ... # same fields as before\n\n    @model_validator(mode=\"after\")\n    def alpha_must_be_double_r(self) -&gt; \"LoraConfigSpec\":\n        \"\"\"Validate the early stopping patience.\"\"\"\n        assert self.lora_alpha == self.r * 2, \\\n            \"Oh no, looks like your alpha value is not twice that or r. Please fix it!\"\n        return self\n</code></pre> <p>You'll see such validators used in many places, as it allows the Training Wizard to fail early and fail loudly - saving you from many bugs and quiet failures that can happen otherwise.</p>"},{"location":"concepts/spec/#specs-inside-specs-inside-specs-inside","title":"Specs inside specs inside specs inside ...","text":"<p>Most specs don't just have fields that are core python objects. They most often contain other specs that manage a configuration of their own. Here's an example from the <code>TrainingSpec</code> class, which is at the core of every recipe in the Training Wizard:</p> <pre><code>class TrainingSpec(...):\n    \"\"\"Specification for a training recipe.\"\"\"\n\n    recipe_class: str\n    \"\"\"The path to the recipe class (`module.submodule.class_name`). Checks that it points to a valid class.\"\"\"\n\n    mlflow_experiment_name: Optional[str] = None\n    \"\"\"The name of the MLflow experiment to log to. If None, do not log to MLflow.\"\"\"\n\n    training_args_spec: TrainingArgumentsSpec\n    \"\"\"The training arguments.\"\"\"\n\n    dataset_spec: Union[DatasetSpec, StreamingDatasetSpec]\n    \"\"\"The dataset to use for training.\"\"\"\n</code></pre> <p>As you can see, Pydantic even supports nesting different specs (or models, as they're called by Pydantic - I think you can see why this is problematic for us), optional specs or even spec unions!</p>"},{"location":"concepts/spec/#config-files-for-nested-specs","title":"Config Files for Nested Specs","text":"<p>Every spec can be uniquely represented by a nested dictionary. More conveniently, though, it can be represented by a <code>toml</code> config using tables. In this example, it would look like this:</p> training_spec_config.toml<pre><code>spec_class = \"training_wizard.recipes.modular.ModularTrainingSpec\"\n\n# Omitting mlflow_experiment_name because we don't want to log yet\n\n[wizard_module]\nspec_class = \"training_wizard.specs.modules.causal_seq2seq.CausalSeq2SeqModule\"\n... # Some fields\n\n[training_args_spec]\n... # Some more fields\n\n[dataset_spec]\n... # Some even more fields\n</code></pre> <p>Unions</p> <p>Fields types as <code>Spec1 | Spec2</code> can be configured with either <code>Spec1</code>s fields or <code>Spec2</code>s fields. Pydantic will automatically pick the right one at runtime.</p>"},{"location":"concepts/spec/#even-more-nesting","title":"Even More Nesting","text":"<p>If <code>training_args_spec</code> had another spec field <code>optimizer_spec</code>, you would define it in the same file using the TOML table syntax:</p> training_spec_config.toml<pre><code>recipe_class = \"training_wizard.recipes.CausalLanguageModeling\"\n\n# Omitting mlflow_experiment_name because we don't want to log yet\n\n[wizard_module]\nspec_class = \"training_wizard.specs.modules.causal_seq2seq.CausalSeq2SeqModule\"\n... # Some fields\n\n[wizard_module.transformer_spec]\n... # Some nested fields\n\n[wizard_module.transformer_spec.tokenizer_init_kwargs]\n... # Some nested nested fields\n\n[training_args_spec]\n... # Some fields again\n\n[dataset_spec]\n... # Some even more fields again\n</code></pre>"},{"location":"concepts/spec/#spec-abstraction-and-inheritance","title":"Spec Abstraction and Inheritance","text":"<p>Just like other class-based architectures, specs in the Training Wizard benefit from abstraction and inheritance for managing complexity and shared functionality. Specs inherit all the fields and validators of their parent class, as expected. </p> <p>All specs also directly or indirectly inherit from BaseModel, which is a Pydantic base class that enables all the Pydantic features in our classes.</p>"},{"location":"concepts/spec/#specs-doing-stuff","title":"Specs Doing Stuff","text":"<p>Most specs will provide different calculated fields in the form of properties (oftentimes <code>cached_property</code> properties which are only calculated once). Additionally, recipes (which are also specs) will always provide a <code>main()</code> method that executes their training routine.</p> <p>Methods are slightly less important, with the exception of the <code>main()</code> method at the core of every recipe.</p>"},{"location":"concepts/toml_config/","title":"TOML Configs","text":"<p>The Training Wizard uses TOML files to define configuration needed to run a training recipe (= training spec). </p> <p>This is called the \"training recipe specification\" of a training run because it specifies exactly how the training run should perform. Example recipe specifications can be found in the <code>examples</code> folder.</p>"},{"location":"develop/","title":"Developer Documentation","text":""},{"location":"develop/coding_conventions/","title":"Coding conventions","text":"<p>Here we collect all the coding conventions and best practices that we want to follow. This mainly concerns Python code, but can also include conventions for other parts of the code.</p>"},{"location":"develop/coding_conventions/#pep8","title":"\ud83d\udcd6 PEP8","text":"<p>Most style questions for Python are governed by PEP8, which we adopt with some exceptions (see below).</p> <p>If you find yourself hard to keep up with PEP8, then you can use the <code>ruff</code> formatter. It allows you to think less about formatting and more about the code you write, as it will auto-format the files you are working on. If you use VS Code, you can install the official ruff extension for VS Code. For more details on a set of recommended extensions, read VS Code tips.</p> <p>\u2934\ufe0f Exceptions to PEP8 rules</p> <ul> <li>The goal of coding conventions is improving readability and maintainability, not uniformity at all costs. Therefore, every rule can be broken when it would otherwise hinder readability. This also may be an argument against using formatters because they do not allow for such flexibility.</li> <li>Line length: 79 characters is quite restrictive and only makes sense when working from small terminal windows or when having multiple editors side by side. For our projects, 120 characters are perfectly fine.</li> </ul>"},{"location":"develop/coding_conventions/#formatting-of-docstrings","title":"\u270d\ufe0f Formatting of docstrings","text":"<p>We use google-style notypes docstrings. They may be used by <code>mkdocstrings</code> to generate API documentation (along with the method's signature).</p> <p>We always use type hints in the method's signature. The types of parameters and return values should not be mentioned in the docstring. It is preferable to only have one source for each piece of information to avoid conflicting information (otherwise when a method's signature changes, both places would need updating). Also it makes the docstring look less verbose.</p> <p>Example</p> <pre><code>def get_token_action(self, token: str, index: int, prob: float, sugg_token: str) -&gt; tuple[int, int, str, float] | None:\n    \"\"\"Get a suggested action for a token.\n\n    This method returns a suggested action for a token.\n    The action is represented as a tuple of 4 elements:\n        starting position, ending position, replacement token, probability of taking the action.\n\n    Args:\n        token: token.\n        index: index of token in sentence.\n        prob: probability of requesting the action.\n        sugg_token: suggested token.\n\n    Raises:\n        ValueError: if `sugg_token` is unknown.\n\n    Returns:\n        Suggested action for token.\n    \"\"\"\n</code></pre>"},{"location":"develop/coding_conventions/#linting","title":"\ud83d\udedf Linting","text":"<p>A linter is a tool that can help you follow PEP8, as well as proper formatting of docstrings and provide warnings for many other cases.</p> <p>We use the ruff linter. If you use VS Code, you can install the official ruff extension for VS Code. For more details on a set of recommended extensions, read VS Code tips.</p> <p>A project using ruff should have it in its development dependencies:</p> <p>Example</p> <pre><code>[dependency-groups]\ndev = [\"ruff&gt;=0.9\"]\n</code></pre> <p>The ruff settings are declared within the <code>pyproject.toml</code> file. Some recommended settings for the ruff linter are specified below.</p> <p>Recommended settings</p> <pre><code>[tool.ruff]\n# Allow lines to be as long as 120.\nline-length = 120\n\n[tool.ruff.lint]\nselect = [\n    \"D\",      # pydocstyle\n    \"W\",      # pycodestyle warnings\n    \"E\",      # pycodestyle errors\n    \"N\",      # pep8-naming\n    \"I\",      # isort\n    \"ANN001\", # Missing type annotation for function argument\n    \"ANN2\",   # Missing type annotation for return value\n    \"F\",      # Pyflakes\n    \"UP\",     # pyupgrade\n    \"RUF\",    # Ruff checks\n    \"A\",      # flake8-builtins (shadowing a Python builtin)\n    \"B\",      # flake8-bugbear\n    \"C4\",     # flake8-comprehensions\n    \"SIM\",    # flake8-simplify\n    \"Q\",      # flake8-quotes\n    \"PIE\",    # flake8-pie\n    \"T10\",    # flake8-debugger\n    \"TCH\",    # flake8-type-checking\n    \"TID\",    # flake8-tidy-imports\n    \"EXE\",    # flake8-executable\n    \"G\",      # flake8-logging-format\n    \"INT\",    # flake8-gettext\n    \"ISC\",    # flake8-implicit-str-concat\n    \"FLY\",    # flynt\n    \"NPY\",    # NumPy-specific rules\n    \"PD\",     # pandas-vet\n    \"PERF\",   # Perflint\n    \"PGH\",    # pygrep-hooks\n]\n\nignore = [\n    \"PGH003\", # we want to just use `type: ignore`\n    \"TID252\", # relative imports\n]\n\n[tool.ruff.lint.per-file-ignores]\n# Ignore `F401` (unused imports) in all `__init__.py` files.\n\"__init__.py\" = [\"F401\"]\n\"**/*.ipynb\" = [\"ALL\"]\n\n[tool.ruff.lint.pydocstyle]\n# Use Google docstring style.\nconvention = \"google\"\n\n[tool.ruff.lint.flake8-annotations]\n# Suppress type annotations for none-returning functions.\nsuppress-none-returning = true\n\n[tool.ruff.format]\n# Line endings will be converted to `\\n`.\nline-ending = \"lf\"\n</code></pre>"},{"location":"develop/coding_conventions/#typing","title":"\ud83c\uddf9 Typing","text":"<p>Python supports type hints via the <code>typing</code> module. It is encouraged to add them as they can help to document methods and functions and spot errors before running your code.</p> <p>We use the new typing syntax that doesn't require the <code>typing</code> module for most things.</p> <p>We always use type hints in the method's signature. The local variables within a function do not need to be annotated, unless it is a complicated case that would increase understanding of the code.</p> <p>In VS Code we use Pylance's basic type checking mode. We will evaluate in the future astral's type checker or mypy.</p> <p>Annotations and co_annotations</p> <p>We do not use \u201cannotations\u201d or \u201cco_annotations\u201d yet for forward references and other cool stuff until they are supported by CPython (without a <code>from __future__</code> import). This will happen in CPython 3.14 and when we make 3.14 our minimum supported version we will use them. For now, we stringify such cases.</p>"},{"location":"develop/coding_conventions/#dependency-management","title":"\ud83d\udd8a\ufe0f Dependency management","text":"<p>For dependency management, we are using uv. It combines the features of Python venv and pip while still managing the dependencies in the standard (PEP 621) <code>pyproject.toml</code> file format in a single location along with a lock file. We are following all the adopted PEPs that are PyPA specifications.</p>"},{"location":"develop/coding_conventions/#future-python-migrations","title":"\ud83d\udc0d Future Python migrations","text":"<p>We use CPython 3.11 as the minimum supported version.</p> <p>If we know that some code can be written better in a newer CPython release, we make a comment that starts with <code># TODO: py</code>  (case-insensitive) and describing how we would have done it in the newer CPython release. e.g. <code># TODO: Python 3.11 porting: Import \"StrEnum\" from enum instead of defining it below.</code>. </p>"},{"location":"develop/coding_conventions/#readme","title":"\ud83d\udcc4 README","text":"<p>The README should contain enough information for a new developer to onboard to the project to the point where they can run some examples in the code. It should at least contain a full installation description.</p> <p>Apart from that, the README should also describe all major features of the project and should be updated accordingly when a new feature is added.</p> <p>Minimal requirements for the README</p> <ul> <li>The file should start with a few sentences that explain what the repository is there for.</li> <li>The README should contain instructions on how to install it and run all the major workflows covered by the repository.</li> <li>These instructions can be minimal in the form of shell snippets. If they are, however, a more detailed explanation for onboarding should be found in additional documents in the <code>docs</code> folder and there should be links to this information in the main README.</li> </ul>"},{"location":"develop/coding_conventions/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>If the repository does not have a <code>CONTRIBUTING.md</code> file yet, assume that this was an oversight due to time constraints and create one. \ud83d\ude09</p> <p>A <code>CONTRIBUTING.md</code> file documents all major development workflows. This does not pertain to installing and running the code in the repo, but to everything one has to keep in mind when adding code to the repository.</p> <p>At the minimum, <code>CONTRIBUTING.md</code> should detail all steps required to release a new version of the repository. If the repository uses CI scripts, these should be explained here.</p>"},{"location":"develop/coding_conventions/#versioning","title":"\ud83c\uddfb Versioning","text":"<p>The versioning scheme of the project depends on how the project is being consumed. There are two cases:</p> <ol> <li>If it is a library that has stable API &amp; CLI assurances defined then it should be SemVer to signal that.<ol> <li>it is good to add a small definition of what are the changes that should trigger a MINOR or a MAJOR version change.</li> </ol> </li> <li>If it is a program that isn't a dependency of another project, then it should be CalVer.</li> </ol>"},{"location":"develop/coding_conventions/#changelog","title":"\ud83d\uddd2\ufe0f Changelog","text":"<p>If the repository does not have a changelog yet, assume that this was an oversight due to time constraints and create one. \ud83d\ude09</p> <p>We use the format from Keep a Changelog.</p> <p>It is good to categorize changes to quickly identify those that may have affected a change or introduced a particular bug. Emojis are a good way to accomplish that since they are immediately recognizable but take up only one character of space. If the changelog uses Emojis for categorization, it should explain the meaning of each symbol at the top.</p> <p>As a general rule, the changelog should be updated every time a pull request is merged into the repository or an issue is closed.</p>"},{"location":"develop/coding_conventions/#unit-tests","title":"\u2705 Unit tests","text":"<p>We use <code>pytest</code> for unit tests.</p> <p>Minimal requirements for unit tests</p> <ul> <li>The repository should have at least one unit test that covers the main workflow of that repository.</li> <li>If a PR introduces code that is not covered by existing unit tests, a new unit test should be added.</li> <li>If a PR introduces a function or class with code that is difficult to understand at first glance, that code should be covered by a unit test.</li> <li>Each unit test needs at least one sentence in its docstring that explains the hypothesis that is being covered by the test.</li> </ul>"},{"location":"develop/coding_conventions/#other-style-guides","title":"\ud83c\udfa9 Other style guides","text":"<ul> <li>When in doubt, the Google style guide for Python code is always a good resource.</li> </ul>"},{"location":"develop/pydantic/","title":"Pydantic troubleshooting","text":"<p>We are using Pydantic in a lot of parts to initialize complex classes from a single TOML file. While this is convenient and works out of the box 95% of the time, the remaining 5% can include errors that are hard to debug because they happen in code that is injected and not called explicitly. This page collects problems we've encountered and possible solutions to speed up their resolution in the future.</p>"},{"location":"develop/pydantic/#i-specified-an-alias-but-now-the-original-field-name-is-not-accepted-anymore","title":"I specified an alias, but now the original field name is not accepted anymore","text":"<p>This is intentional (albeit misleading). The Pydantic documentation specifies the following:</p> <p>An alias is an alternative name for a field, used when serializing and deserializing data.</p> <p>What this means is that the alias name and only the alias name of the field will be used for serializing and deserializing. If you want alternatives, you have to use AliasChoices instead:</p> <pre><code>class User(BaseModel):\n    first_name: str = Field(validation_alias=AliasChoices('first_name', 'fname'))\n    last_name: str = Field(validation_alias=AliasChoices('last_name', 'lname'))\n</code></pre>"},{"location":"develop/pydantic/#i-have-a-field-with-a-union-type-but-pydantic-initializes-the-wrong-type","title":"I have a field with a union type, but Pydantic initializes the wrong type","text":"<p>Pydantic is pretty crafty at automatically resolving complex union types in fields, but sometimes it needs a bit of help. One prominent example where this is the case is when <code>before</code> validators are involved.</p> <p>Consider the following example:</p> <pre><code>from pydantic import BaseModel, model_validator\n\nclass TypeA(BaseModel):\n    a: int\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_fields(cls, values):\n        print(values[\"a\"])\n        return values\n\nclass TypeB(BaseModel):\n    b: int\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_fields(cls, values):\n        print(values[\"b\"])\n        return values\n\nclass MyClass(BaseModel):\n    first_field: TypeA | TypeB\n\ndata = {\"first_field\": {\"a\": 1}}\nprint(MyClass(**data))\n</code></pre> <p>This looks innocent, but will break because the validators of both classes that are part of the union type will be called. In other cases, this can even lead to Pydantic trying to initialize the wrong class. The reason here is the <code>@model_validator(mode=\"before\")</code>. Since we specify <code>mode=\"before\"</code>, Pydantic has not yet resolved the Union type. We are just operating on raw dicts here, so it will try to validate <code>first_field</code> both as <code>TypeA</code> and <code>TypeB</code>, one of which will always fail, of course.</p> <p>The solution for this problem depends on the specific situation, but in the above example, this can be resolved by providing an explicit <code>Discriminator</code> in the union type that helps to distinguish which type to use both before and after initialization:</p> <pre><code>from typing import Annotated\nfrom pydantic import BaseModel, Discriminator, Tag, model_validator\n\nclass TypeA(BaseModel):\n    a: int\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_fields(cls, values):\n        print(values[\"a\"])\n        return values\n\nclass TypeB(BaseModel):\n    b: int\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_fields(cls, values):\n        print(values[\"b\"])\n        return values\n\ndef get_discriminator_value(value):\n    if isinstance(value, dict) and \"a\" in value:\n        return \"alpha\"\n    elif isinstance(value, dict) and \"b\" in value:\n        return \"beta\"\n    elif isinstance(value, TypeA):\n        return \"alpha\"\n    elif isinstance(value, TypeB):\n        return \"beta\"\n\nclass MyClass(BaseModel):\n    first_field: Annotated[\n        Annotated[TypeA, Tag(\"alpha\")] | Annotated[TypeB, Tag(\"beta\")],\n        Discriminator(get_discriminator_value),\n    ]\n\ndata = {\"first_field\": {\"a\": 1}}\nprint(MyClass(**data))\n</code></pre>"},{"location":"develop/regex-python/","title":"Regular Expressions in Python","text":"<p>A tutorial on Regular Expressions and their usage in Python</p> <ul> <li>A Visual Guide to Regular Expression</li> </ul> <p>Official Python documentation on Regular Expressions</p> <ul> <li>Regular Expression HOWTO</li> <li>re \u2014 Regular expression operations</li> </ul> <p>A \u201cweb IDE\u201d for Regular Expressions</p> <ul> <li>regex101: build, test, and debug regex</li> </ul> <p>Adapt the \u201cweb IDE\u201d for python</p> <ul> <li>Select \u201cPython\u201d as your Flavor on the left sidebar.</li> <li>Go to Settings and select Python as your default flavor and your default code generator language.</li> </ul>"},{"location":"develop/vs-code/","title":"VS Code tips","text":"<p>The following consist of optional suggestions for configuring VS Code as well as for installing helpful extensions to it. If you use VS Code as your code editor for contributing to this project you may find them useful.</p>"},{"location":"develop/vs-code/#settings","title":"Settings","text":"<p>An easy way to add settings to VS Code is to declare them in the JSON settings file (F1 \u2192 Open User Settings (JSON)). However, if you are unsure about messing with JSON, you can also search for the name of the individual settings in the normal settings UI (F1 \u2192 Open User Settings).</p> <pre><code>\"files.autoSave\": \"afterDelay\",\n\"editor.rulers\": [\n    120\n],\n\"python.analysis.typeCheckingMode\": \"basic\",\n\"github.gitProtocol\": \"ssh\",\n\"python.testing.pytestEnabled\": true,\n\"autoDocstring.docstringFormat\": \"google-notypes\",\n\"evenBetterToml.schema.enabled\": false,\n</code></pre> <p>Layers of settings</p> <p>There are different layers of settings. Depending on the scope you want these settings to have, you can apply them at the \u201cUser\u201d level, \u201cRemote\u201d, \u201cWorkspace\u201d or in a specific folder.</p> <p>Hierarchy of layers of settings</p> <ul> <li> <p>As mentioned, there are different levels of settings. The hierarchy is as follows: User \u2192 Remote \u2192 Workspace \u2192 Folder. The User settings are on top and apply globally to any instance of VS Code you open.</p> </li> <li> <p>Each level down becomes more specific, with its settings overriding those of \u201chigher levels\u201d. So, if you change a value in the Remote settings that also exists in the User settings, the Remote value will take precedence. Of course, this value would only apply to the currently open Remote instance.</p> </li> <li>Similarly with Workspace and Folder. A Workspace is a project you have open. Note that workspace settings are stored at the root of the project in a\u00a0<code>.vscode</code>\u00a0folder.</li> </ul> <p>Settings for extensions</p> <p>Some of these settings apply to optional extensions. If you don't have those extensions, these settings will just be ignored. You can still safely copy the string below into your settings file.</p>"},{"location":"develop/vs-code/#extensions","title":"Extensions","text":""},{"location":"develop/vs-code/#core-extensions","title":"Core extensions","text":"<p>We list these extensions with their unique identifier (like <code>ms-python.python</code>), so that you can easily install them from the command line or add them to the <code>remote.SSH.defaultExtensions</code> setting. You can also use that identifier to find the extension in the normal extension view (F1 \u2192 Extensions: Install Extensions).</p> <p>Install via CLI:</p> <pre><code>code --install-extension ms-python.python                              # Python support\ncode --install-extension ms-vscode-remote.vscode-remote-extensionpack  # Remote Development extension pack\ncode --install-extension GitHub.vscode-pull-request-github             # GitHub Pull Requests and Issues\n</code></pre> <p>Add to default extensions (F1 \u2192 Open User Settings (JSON)):</p> <pre><code>\"remote.SSH.defaultExtensions\": [\n  \"ms-python.python\",\n  \"GitHub.vscode-pull-request-github\",\n]\n</code></pre>"},{"location":"develop/vs-code/#useful-extensions","title":"Useful extensions","text":"<p>These are not mandatory, but helpful in a lot of cases.</p> <p>Install via CLI:</p> <pre><code>code --install-extension ms-vscode-remote.vscode-remote-extensionpack  # Remote Development extension pack\ncode --install-extension ms-python.python                   # Python extension\ncode --install-extension ms-toolsai.jupyter                 # Jupyter extension\ncode --install-extension charliermarsh.ruff                 # Ruff linter extension\ncode --install-extension eamodio.gitlens                    # GitLens extension (code insights via git history)\ncode --install-extension mhutchie.git-graph                 # A graphical git log\ncode --install-extension rioj7.command-variable             # Needed for the \"Current Module\" debug profile\ncode --install-extension ryu1kn.partial-diff                # Create diff of sections within files\ncode --install-extension njpwerner.autodocstring            # Generate docstrings from function signature\ncode --install-extension kevinrose.vsc-python-indent        # Helps getting Python indentations right\ncode --install-extension GitHub.vscode-pull-request-github  # GitHub Pull Requests and Issues\ncode --install-extension GitHub.copilot                     # GitHub Copilot\ncode --install-extension GitLab.gitlab-workflow             # GitLab Workflow\ncode --install-extension mtxr.sqltools                      # Facilitates working with SQL databases\ncode --install-extension mtxr.sqltools-driver-mysql         # MySQL driver for SQLTools extension\ncode --install-extension ms-azuretools.vscode-docker        # Docker support for VS Code\ncode --install-extension tamasfe.even-better-toml           # TOML Language Support\ncode --install-extension redhat.vscode-xml                  # XML Language Support\ncode --install-extension redhat.vscode-yaml                 # YAML Language Support\ncode --install-extension yzhang.markdown-all-in-one         # Markdown Language Support\ncode --install-extension howcasperwhat.comment-formula      # LaTeX formulas in comments rendered in pop-up\ncode --install-extension bierner.emojisense                 # Search for emojis\ncode --install-extension brunnerh.insert-unicode            # Search for Unicode characters\ncode --install-extension GrapeCity.gc-excelviewer           # Spreadsheet Viewer for csv/tsv/xls files\ncode --install-extension bierner.markdown-mermaid           # Supports Mermaid diagrams in Markdown preview\ncode --install-extension ms-vscode.wordcount                # Adds word count to status bar\n</code></pre> <p>If you use a remote server it is not necessary to do this again in the remote server. Add to SSH default extensions when using a remote server (F1 \u2192 Open User Settings (JSON)):</p> <pre><code>\"remote.SSH.defaultExtensions\": [\n    \"ms-python.python\",\n    \"ms-toolsai.jupyter\",\n    \"charliermarsh.ruff\",\n    \"eamodio.gitlens\",\n    \"mhutchie.git-graph\",\n    \"rioj7.command-variable\",\n    \"ryu1kn.partial-diff\",\n    \"njpwerner.autodocstring\",\n    \"kevinrose.vsc-python-indent\",\n    \"GitHub.vscode-pull-request-github\",\n    \"GitHub.copilot\",\n    \"GitLab.gitlab-workflow\",\n    \"mtxr.sqltools\",\n    \"mtxr.sqltools-driver-mysql\",\n    \"ms-azuretools.vscode-docker\",\n    \"tamasfe.even-better-toml\",\n    \"redhat.vscode-xml\",\n    \"redhat.vscode-yaml\",\n    \"yzhang.markdown-all-in-one\",\n    \"howcasperwhat.comment-formula\",\n    \"bierner.emojisense\",\n    \"brunnerh.insert-unicode\",\n    \"valentjn.vscode-ltex\",\n    \"GrapeCity.gc-excelviewer\",\n    \"bierner.markdown-mermaid\",\n    \"ms-vscode.wordcount\"\n]\n</code></pre>"},{"location":"home/usage/","title":"Running a Recipe","text":"<p>This page will walk you through the process of setting up your desired recipe and running the <code>training-wizard</code> command. It is a high-level overview of how you would use the Training Wizard in practice.</p>"},{"location":"home/usage/#make-your-data-accessible","title":"Make your data accessible","text":"<p>The one thing binding all training recipes is the presence of a dataset. Your data must be accessible through the loading utility of the <code>datasets</code> library, meaning it should be either:</p> <ul> <li> <p>Available on the HuggingFace Hub</p> </li> <li> <p>Saved locally in a compatible format (<code>txt</code>, <code>csv</code>, <code>json</code>, <code>jsonl</code>, <code>arrow</code>, <code>sql</code>, etc...)</p> </li> </ul> <p>That's it for now. Further down, we'll talk about how to inject preprocessing steps into your chosen recipe to bring your dataset columns into the required format.</p>"},{"location":"home/usage/#set-up-your-toml-config","title":"Set up your TOML config","text":"<p>Configuring your recipe might work a bit differently than you're used to. Because of the way the Training Wizard is structured, building a config happens in a somewhat recursive fashion.</p> <p>Nested Specs</p> <p>The spec explanation also teaches you how to build a config from scratch.</p> <p>Experiment Tracking</p> <p>You can use MLflow to track your experiments you do with the Training Wizard. To activate MLflow for a specific experiment, you need to add an <code>mlflow_experiment_name</code> to your TOML file, such as <code>mlflow_experiment_name = \"test_mlflow\"</code>, which can be found in the demo TOML file.</p>"},{"location":"home/usage/#config-examples","title":"Config examples","text":"<p>The <code>examples</code> directory contains demo TOML configs for various recipes that can be used as a reference and is a good starting point.</p>"},{"location":"home/usage/#run-the-recipe","title":"Run the recipe","text":"<p>Once you have your TOML config set up, you're good to go! </p> <p>Run the Wizard and start training with the command: </p> <pre><code>training-wizard path/to/your/config.toml\n</code></pre> <p>Tip</p> <p>The training process may take a while (hours or days) to finish. You may want to run this command in a <code>tmux</code> session.</p>"},{"location":"home/usage/#multi-gpu-training","title":"Multi-GPU Training","text":"<p>For distributed training, use <code>accelerate</code>. First configure your setup:</p> <pre><code>accelerate config\n</code></pre> <p>Then launch any recipe with:</p> <pre><code>accelerate launch -m training_wizard path/to/your/config.toml\n</code></pre>"}]}